========================================
Total Params: 71891291
Trainable Params: 71891291
Non-trainable Params: 0
========================================
LOG FORMAT | segmentation_LOSS mIoU pixAcc | depth_LOSS abs_err rel_err | normal_LOSS mean median <11.25 <22.5 <30 | TIME
Epoch 0/199
self_multiinpout False
753
tensor([3.0449, 0.7070, 1.0451], device='cuda:0', grad_fn=<CopySlices>)
  0%|                                                                                                                                           | 0/753 [00:00<?, ?it/s]

  0%|▎                                                                                                                                  | 2/753 [00:03<20:49,  1.66s/it]

  0%|▌                                                                                                                                  | 3/753 [00:04<20:31,  1.64s/it]

  1%|▋                                                                                                                                  | 4/753 [00:06<20:55,  1.68s/it]
tensor([2.4262, 0.6500, 0.9558], device='cuda:0', grad_fn=<CopySlices>)


  1%|█                                                                                                                                  | 6/753 [00:09<20:38,  1.66s/it]
tensor([2.0938, 0.6828, 0.7376], device='cuda:0', grad_fn=<CopySlices>)
  1%|█▍                                                                                                                                 | 8/753 [00:13<21:42,  1.75s/it]
Traceback (most recent call last):
  File "examples/warehouseSIM/train_warehouseSIM.py", line 126, in <module>
    main(params)
  File "examples/warehouseSIM/train_warehouseSIM.py", line 118, in main
    NYUmodel.train(nyuv2_train_loader, nyuv2_test_loader, 200)
  File "/home/dim/mdpi_robotics/LibMTL/LibMTL/trainer.py", line 230, in train
    self.optimizer.step()
  File "/home/dim/anaconda3/envs/libmtl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/dim/anaconda3/envs/libmtl/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/home/dim/anaconda3/envs/libmtl/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/dim/anaconda3/envs/libmtl/lib/python3.8/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/home/dim/anaconda3/envs/libmtl/lib/python3.8/site-packages/torch/optim/_functional.py", line 81, in adam
    grad = grad.add(param, alpha=weight_decay)
KeyboardInterrupt